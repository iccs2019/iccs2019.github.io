---
title: Dr. James Anderson
name: James-Anderson
subtitle: Professor of Cognitive, Linguistic and Psychological Sciences at Brown University
layout: 2017_default
modal-id: 1
img: James-Anderson.jpg
thumbnail: James-Anderson.jpg
alt: Picture of Dr. James Anderson
topic: From Aristotle to William James to Deep Learning  &#58; Everything Old is New Again.
description: The most influential models for human mental function are based on association.  This suggestion was first made by Aristotle, in his role as data-based psychologist, not Aristotle as logician.  He assumed that the basic units of memory are complex sensory objects and these objects can become linked together by experience and learning rules.  The resulting network of associations can be creative and the ability to compute with networks of associations is what makes us human.  Explicit teaching is helpful.  A pedagogical classic (Plato’s Meno) has Socrates showing how an uneducated servant boy can use associative steps and perceptual knowledge to solve an abstract geometrical problem.  In the late 19th Century, William James was famously concerned with association.  James proposed a Hebbian learning network that is essentially identical to modern simple neural networks.  James also carefully pointed out good and bad features of association as a practical computing mechanism.<br><br>Good Things&#58;  Association is mechanistic, hardware driven, uses explicit learning rules, and has natural signal processing abilities.  Association works directly with large, complex cognitive objects and can be taught, programmed, and controlled.  It can display network driven creativity.  It is "alogical", that is, works independently of logic.<br><br>Bad Things&#58; Association has major difficulties with ambiguity.  It is imprecise in practice. (But imprecision is sometimes a bug and sometimes a feature.)  It is inflexible and “boring” without the use of programming tools to control the direction taken by the associations. <br><br> Most current neural network models, including deep learning, are associative in structure but rarely make use the potential of association to be flexible, teachable, and programmable.  Though deep learning is a powerful engineering technology, learning millions of examples is not how humans do it. <br><br>The last part of the talk mixes association with ideas from cortical neuroscience&#58; columns, travelling waves, and topographic maps.  The details of the underlying neural hardware directly affect the performance of the program and allow for experimental tests.  We start by discussing how "abstract" concepts such as identity, symmetry, and number can arise.  We then use this structure to construct a speculative "brain-like" neural network program for five basic numerical operations.  The resulting program architecture is a hybrid, part analog, part a non-linear dynamical system. <br><br><b>Brief Biography &#58;</b>James Anderson grew up in Los Angeles, California when it was still a livable, enjoyable backwater.  He went to MIT and received an S.B. in physics and a Ph.D. in neurophysiology.  He was a post-doc at the UCLA Brain Research Institute and Rockefeller University.  He has been Assistant, Associate, and now Professor at Brown University, Providence, Rhode Island and is currently Professor in the Department of Cognitive, Linguistic, and Psychological Sciences.  He has been involved with several books and many papers in the area of neural networks and their application to computation and cognition.  His most recent book from Oxford University Press is “After Digital&#58; Computation in Brains and Machines” which contains material related to this talk.
---